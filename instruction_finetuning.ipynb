{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\355M\\checkpoint\n",
      "File already exists and is up-to-date: gpt2\\355M\\encoder.json\n",
      "File already exists and is up-to-date: gpt2\\355M\\hparams.json\n",
      "File already exists and is up-to-date: gpt2\\355M\\model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2\\355M\\model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2\\355M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2\\355M\\vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=\"355M\", models_dir=\"gpt2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "    \"gpt2-medium (355M)\": {\n",
    "        \"vocab_size\": 50257,\n",
    "        \"context_length\": 1024,\n",
    "        \"emb_dim\": 1024,\n",
    "        \"n_heads\": 16,\n",
    "        \"n_layers\": 24,\n",
    "        \"drop_rate\": 0.1,\n",
    "        \"qkv_bias\": True\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MyGPT2.gpt2_model import GPT2Model\n",
    "from MyGPT2.train_utils import load_weights_into_gpt\n",
    "\n",
    "model = GPT2Model(\n",
    "    model_configs[\"gpt2-medium (355M)\"]\n",
    ")\n",
    "\n",
    "load_weights_into_gpt(model, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 3.067, Val loss 3.056\n",
      "Ep 1 (Step 000005): Train loss 1.365, Val loss 1.365\n",
      "Ep 1 (Step 000010): Train loss 1.102, Val loss 1.193\n",
      "Ep 1 (Step 000015): Train loss 1.055, Val loss 1.144\n",
      "Ep 1 (Step 000020): Train loss 0.998, Val loss 1.082\n",
      "Ep 1 (Step 000025): Train loss 0.925, Val loss 1.060\n",
      "Ep 1 (Step 000030): Train loss 0.983, Val loss 1.032\n",
      "Ep 1 (Step 000035): Train loss 0.907, Val loss 1.001\n",
      "Ep 1 (Step 000040): Train loss 0.882, Val loss 0.987\n",
      "Ep 1 (Step 000045): Train loss 0.823, Val loss 0.972\n",
      "Ep 1 (Step 000050): Train loss 0.721, Val loss 0.956\n",
      "Ep 1 (Step 000055): Train loss 0.778, Val loss 0.941\n",
      "Ep 1 (Step 000060): Train loss 0.791, Val loss 0.938\n",
      "Ep 1 (Step 000065): Train loss 0.798, Val loss 0.917\n",
      "Ep 1 (Step 000070): Train loss 0.713, Val loss 0.917\n",
      "Ep 1 (Step 000075): Train loss 0.789, Val loss 0.912\n",
      "Ep 1 (Step 000080): Train loss 0.645, Val loss 0.901\n",
      "Ep 1 (Step 000085): Train loss 0.726, Val loss 0.886\n",
      "Ep 1 (Step 000090): Train loss 0.731, Val loss 0.872\n",
      "Ep 1 (Step 000095): Train loss 0.710, Val loss 0.867\n",
      "Ep 1 (Step 000100): Train loss 0.656, Val loss 0.864\n",
      "Ep 1 (Step 000105): Train loss 0.612, Val loss 0.860\n",
      "Ep 1 (Step 000110): Train loss 0.636, Val loss 0.850\n",
      "Ep 1 (Step 000115): Train loss 0.641, Val loss 0.839\n"
     ]
    }
   ],
   "source": [
    "from MyGPT2.finetune_data_utils import create_data_loader\n",
    "import tiktoken\n",
    "from MyGPT2.train_utils import train_model_simple\n",
    "import torch\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "train_loader, test_loader, val_loader = create_data_loader(\n",
    "    file_path=\"instruction-data.json\",\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=0.00005, weight_decay=0.1\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_losses, val_losses, token_seen = train_model_simple(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    optimizer=optimizer,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    num_epochs=1,\n",
    "    eval_freq=5,\n",
    "    eval_iter=5,\n",
    "    start_context=None,\n",
    "    tokenizer=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n",
      "           257,  2882,   326, 20431, 32543,   262,  2581,    13,   198,   198,\n",
      "         21017, 46486,    25,   198, 30003,  6525,   262,  1708,  6827,   284,\n",
      "          4781, 49052,    13,   198,   198, 21017, 23412,    25,   198,  1212,\n",
      "           318,  1107,   257,  1049,   290,  7932,   905,    13,   198,   198,\n",
      "         21017, 18261,    25,   198,  1212,   318,   257,  1049,   290,  7932,\n",
      "           905,    13, 50256,   464,  1708,   318,   281, 12064,   326,  8477,\n",
      "           257,  4876,    13, 19430,   257,  2882,   326, 20431, 32543,   262,\n",
      "          2581,    13,   198,   198, 21017, 46486,    25,   198, 30003,  6525,\n",
      "           262,  6827,  1262,   257,   985,   576,    13,   198,   198, 21017,\n",
      "         18261,    25,   198,   464,  3797, 11687,   625,   262, 13990,    13,\n",
      "         50256,   464,  1708,   318,   281, 12064,   326,  8477,   257,  4876,\n",
      "            13, 19430,   257,  2882,   326, 20431, 32543,   262,  2581,    13,\n",
      "           198,   198, 21017, 46486,    25,   198,  2061,   318,   262,  3139,\n",
      "           286,   262,  1578,  1829,    30,   198,   198, 21017]])\n"
     ]
    }
   ],
   "source": [
    "from MyGPT2.finetune_data_utils import format_input\n",
    "from MyGPT2.text_utils import (\n",
    "    generate_text_simple,\n",
    "    text_to_ids\n",
    ")\n",
    "\n",
    "input = {\n",
    "        \"instruction\": \"Rewrite the following sentence to remove redundancy.\",\n",
    "        \"input\": \"This is really a great and wonderful show.\"\n",
    "    }\n",
    "input = format_input(input)\n",
    "\n",
    "output = generate_text_simple(\n",
    "    model,\n",
    "    text_to_ids(input, tokenizer),\n",
    "    100,\n",
    "    model_configs[\"gpt2-medium (355M)\"][\"context_length\"]\n",
    ")\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nRewrite the following sentence to remove redundancy.\\n\\n### Input:\\nThis is really a great and wonderful show.\\n\\n### Response:\\nThis is a great and wonderful show.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nRewrite the sentence using a simile.\\n\\n### Response:\\nThe cat jumped over the fence.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat is the capital of the United States?\\n\\n###'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from MyGPT2.text_utils import ids_to_text\n",
    "ids_to_text(output, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = \"gpt2_instruct.pth\"\n",
    "torch.save(model.state_dict(), SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
